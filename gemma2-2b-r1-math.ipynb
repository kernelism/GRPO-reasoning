{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"},{"sourceId":14474742,"sourceType":"datasetVersion","datasetId":9245295},{"sourceId":85992,"sourceType":"modelInstanceVersion","modelInstanceId":72251,"modelId":76277}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GSM8K Reasoning Enhancement: SFT + GRPO Pipeline\n\n## Training Strategy\n\n**Stage 1 - SFT (1 Epoch)**  \nTeach format using 5K samples (3K B + 1K C + 1K A). Single epoch to avoid overfitting.\n\n**Stage 2 - GRPO (2 Epochs over 7.4K samples)**  \nUse entire GSM8K dataset with curriculum learning:\n- Epoch 1: Easy problems first (SFT data, model succeeds)\n- Epoch 2: Hard problems (base model failures)\n\n## Data Strategy: 3K/1K/1K Split\n\n| Type | Count | Description |\n|------|-------|-------------|\n| **B** | 3,000 | Rich chain-of-thought |\n| **C** | 1,000 | Alternative reasoning |\n| **A** | 1,000 | Concise baseline |\n\n---\n**Hardware**: TPU v5e-8","metadata":{}},{"cell_type":"code","source":"!pip install -q \"numpy<2\" \"flax==0.12.0\" \"google-tunix[prod]==0.1.3\"\n!pip install -q kagglehub grain datasets pandas huggingface_hub\n\nimport jax\nprint(f\"JAX {jax.__version__} | {jax.device_count()} devices\")\nassert jax.device_count() == 8, \"Requires TPU v5e-8\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:24:23.697731Z","iopub.execute_input":"2026-01-14T03:24:23.697874Z","iopub.status.idle":"2026-01-14T03:25:31.050857Z","shell.execute_reply.started":"2026-01-14T03:24:23.697856Z","shell.execute_reply":"2026-01-14T03:25:31.0497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, gc, re, json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom flax import nnx\nimport grain\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nimport qwix\n\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.models.gemma import model as gemma_lib\nfrom tunix.models.gemma import params as params_lib\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft.peft_trainer import TrainingInput\n\nMESH = [(2, 4), (\"fsdp\", \"tp\")]\nLORA_RANK, LORA_ALPHA = 64, 64.0\n\n# SFT: 1 epoch only\nSFT_EPOCHS = 1\nSFT_BATCH = 4\nSFT_LR = 1e-5\nMAX_SEQ = 1024\n\n# GRPO: 1 epochs over full 7.4K\nGRPO_EPOCHS = 1\nGRPO_BATCH = 2\nGRPO_LR = 3e-6\nNUM_GENS = 4\nKL_COEF = 0.08\n\nWORK_DIR = Path(\"/kaggle/working\")\nCKPT_DIR = WORK_DIR / \"ckpts\"\nSFT_CKPT = CKPT_DIR / \"sft\"\nGRPO_CKPT = CKPT_DIR / \"grpo\"\nfor p in [CKPT_DIR, SFT_CKPT, GRPO_CKPT]: p.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:25:31.051495Z","iopub.execute_input":"2026-01-14T03:25:31.051771Z","iopub.status.idle":"2026-01-14T03:25:46.975436Z","shell.execute_reply.started":"2026-01-14T03:25:31.051744Z","shell.execute_reply":"2026-01-14T03:25:46.974388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nos.environ[\"KAGGLE_USERNAME\"] = secrets.get_secret(\"kaggle_username\")\nos.environ[\"KAGGLE_KEY\"] = secrets.get_secret(\"kaggle_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:25:46.976063Z","iopub.execute_input":"2026-01-14T03:25:46.976393Z","iopub.status.idle":"2026-01-14T03:25:47.132838Z","shell.execute_reply.started":"2026-01-14T03:25:46.976376Z","shell.execute_reply":"2026-01-14T03:25:47.132014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data: 3K/1K/1K for SFT, Full 7.4K for GRPO\ntry:\n    ds_path = kagglehub.dataset_download(\"bazingawaggle/gsm8k-merged\")\n    df = pd.read_csv(f\"{ds_path}/gsm8k_merged.csv\")\nexcept:\n    from datasets import load_dataset\n    raw = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n    df = pd.DataFrame([{\"question\": x[\"question\"], \"gold\": x[\"answer\"].split(\"####\")[-1].strip(), \"has_B\": True, \"has_C\": False} for x in raw])\n\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# SFT: 3K B + 1K C + 1K A\nsft_samples, used_ids = [], set()\n\nfor idx in df[df.get('has_B', True) == True].index[:3000]:\n    used_ids.add(idx); row = df.loc[idx]\n    sft_samples.append({\"q\": row[\"question\"], \"a\": row.get('formatted_B', f\"<reasoning>Solving step by step.</reasoning><answer>{row['gold']}</answer>\"), \"type\": \"B\"})\n\nfor idx in df[(df.get('has_C', False) == True) & (~df.index.isin(used_ids))].index[:1000]:\n    used_ids.add(idx); row = df.loc[idx]\n    sft_samples.append({\"q\": row[\"question\"], \"a\": row.get('formatted_C', f\"<reasoning>Alternative approach.</reasoning><answer>{row['gold']}</answer>\"), \"type\": \"C\"})\n\nfor idx in df[~df.index.isin(used_ids)].index[:1000]:\n    used_ids.add(idx); row = df.loc[idx]\n    a = row.get('formatted_A', row.get('reasoning_A', f\"<reasoning>Direct calculation.</reasoning><answer>{row['gold']}</answer>\"))\n    sft_samples.append({\"q\": row[\"question\"], \"a\": a if pd.notna(a) else f\"<reasoning>Direct.</reasoning><answer>{row['gold']}</answer>\", \"type\": \"A\"})\n\nprint(f\"SFT: {len(sft_samples)} (B:{len([x for x in sft_samples if x['type']=='B'])}, C:{len([x for x in sft_samples if x['type']=='C'])}, A:{len([x for x in sft_samples if x['type']=='A'])})\")\n\n# GRPO: Full dataset with curriculum (Easy first, Hard later)\neasy = df[df.get('has_B', True) == True].copy()\nhard = df[df.get('has_C', False) == True].copy()\nrest = df[~df.index.isin(easy.index) & ~df.index.isin(hard.index)].copy()\ngrpo_curriculum = pd.concat([easy, rest, hard]).drop_duplicates('question').reset_index(drop=True)\n\ngrpo_samples = [{\"q\": r[\"question\"], \"gold\": str(r[\"gold\"])} for _, r in grpo_curriculum.iterrows()]\nprint(f\"GRPO: {len(grpo_samples)} samples (Easy→Medium→Hard curriculum)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:25:47.133387Z","iopub.execute_input":"2026-01-14T03:25:47.133562Z","iopub.status.idle":"2026-01-14T03:25:48.155392Z","shell.execute_reply.started":"2026-01-14T03:25:47.133538Z","shell.execute_reply":"2026-01-14T03:25:48.15438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nmodel_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")\nparams = params_lib.load_and_format_params(f\"{model_path}/gemma2-2b-it\")\nmesh = jax.make_mesh(*MESH)\n\nbase_model = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\nwith mesh:\n    state = nnx.state(base_model)\n    nnx.update(base_model, jax.lax.with_sharding_constraint(state, nnx.get_partition_spec(state)))\n\n# ✅ Fixed: Added tokenizer_type argument\ntokenizer = tokenizer_lib.Tokenizer(\"sentencepiece\", f\"{model_path}/tokenizer.model\")\ndel params; gc.collect()\n\nlora_cfg = qwix.LoraProvider(module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\", rank=LORA_RANK, alpha=LORA_ALPHA)\nmodel = qwix.apply_lora_to_model(base_model, lora_cfg, **base_model.get_model_input())\nwith mesh:\n    state = nnx.state(model)\n    nnx.update(model, jax.lax.with_sharding_constraint(state, nnx.get_partition_spec(state)))\nprint(\"Model ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:25:48.156044Z","iopub.execute_input":"2026-01-14T03:25:48.156221Z","iopub.status.idle":"2026-01-14T03:26:14.422598Z","shell.execute_reply.started":"2026-01-14T03:25:48.156203Z","shell.execute_reply":"2026-01-14T03:26:14.421547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stage 1: SFT (1 Epoch)","metadata":{}},{"cell_type":"code","source":"CHAT_TEMPLATE = \"<start_of_turn>user\\n{q}<end_of_turn>\\n<start_of_turn>model\\n{a}<end_of_turn>\"\n\ndef make_sft_input(ex):\n    toks = tokenizer.encode(CHAT_TEMPLATE.format(q=ex['q'], a=ex['a']))\n    seq_len = len(toks)\n    toks = toks[:MAX_SEQ] if seq_len > MAX_SEQ else toks + [0]*(MAX_SEQ - seq_len)\n    arr = np.array(toks, dtype=np.int32)\n    \n    # Positions array\n    positions = np.arange(MAX_SEQ, dtype=np.int32)\n    \n    # Input mask (1 for real tokens, 0 for padding)\n    input_mask = (arr != 0).astype(np.float32)\n    \n    # Causal attention mask for autoregressive training\n    # Shape: (seq_len, seq_len) - lower triangular matrix\n    causal_mask = np.tril(np.ones((MAX_SEQ, MAX_SEQ), dtype=np.float32))\n    \n    return {\n        \"input_tokens\": arr,\n        \"positions\": positions,\n        \"attention_mask\": causal_mask,\n        \"input_mask\": input_mask\n    }\n\nsft_ds = grain.MapDataset.source(sft_samples).map(make_sft_input).batch(SFT_BATCH)\n\nfrom tunix.sft.peft_trainer import PeftTrainer, TrainingConfig\nprint(f\"--- SFT: {len(sft_samples)} samples, {SFT_EPOCHS} epoch ---\")\n\n# Calculate total steps\ntotal_steps = (len(sft_samples) // SFT_BATCH) * SFT_EPOCHS\n\n# Create optimizer\noptimizer = optax.adamw(learning_rate=SFT_LR)\n\n# Create TrainingConfig\ntraining_config = TrainingConfig(\n    eval_every_n_steps=100,\n    max_steps=total_steps,\n    checkpoint_root_directory=str(SFT_CKPT),\n    data_sharding_axis=(\"fsdp\",),\n    pbar_description=\"SFT Training\"\n)\n\n# Create PeftTrainer instance\ntrainer = PeftTrainer(\n    model=model,\n    optimizer=optimizer,\n    training_config=training_config\n)\n\n# Run training\ntrainer.train(sft_ds)\n\n# Save checkpoint\nckptr = ocp.StandardCheckpointer()\nckptr.save(str(SFT_CKPT / \"final\"), nnx.state(model, nnx.LoRAParam))\nckptr.wait_until_finished()\nprint(\"SFT complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T03:29:55.851875Z","iopub.execute_input":"2026-01-14T03:29:55.852272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stage 2: GRPO (1 Epochs, Full 7.4K, Curriculum)","metadata":{}},{"cell_type":"code","source":"# THE COMPLETE REWARD FUNCTION\n#\n# Components:\n#   1. Correctness: +3.0 if exact match, else penalty\n#   2. Format (Strict): +2.0 if valid XML structure\n#   3. Format (Soft): +0.1 per valid tag (partial credit)\n#   4. Reasoning Quality: +0.5 if reasoning contains math operations\n\ndef extract_answer_num(text):\n    \"\"\"Extract numeric answer from <answer> tags or **Answer:** format\"\"\"\n    # Try <answer> tags first\n    m = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)\n    if m:\n        nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", m.group(1))\n        if nums: return float(nums[-1])\n    # Fallback: **Answer:** markdown format\n    m = re.search(r'\\*\\*Answer[:\\*]*\\s*(.+?)(?:\\n|$)', text, re.IGNORECASE)\n    if m:\n        nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", m.group(1))\n        if nums: return float(nums[-1])\n    # Last resort: final number in text\n    nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n    return float(nums[-1]) if nums else None\n\ndef check_ascii_math(text):\n    \"\"\"Check if reasoning contains mathematical operations\"\"\"\n    math_ops = ['+', '-', '*', '/', '=', '%']\n    math_words = ['calculate', 'compute', 'multiply', 'divide', 'add', 'subtract', 'total', 'sum', 'equals']\n    has_ops = any(op in text for op in math_ops)\n    has_words = any(w in text.lower() for w in math_words)\n    return has_ops or has_words\n\n_ITER = [0]  # mutable counter\n\ndef gsm8k_reward_final(prompts, completions, answer, **kwargs):\n    \"\"\"Complete reward function with all components\"\"\"\n    _ITER[0] += 1\n    rewards = []\n    \n    for i, (prompt, completion, gold) in enumerate(zip(prompts, completions, answer)):\n        # Parse gold answer\n        try:\n            gold_num = float(re.findall(r\"[-+]?\\d*\\.?\\d+\", str(gold))[-1])\n        except:\n            gold_num = None\n        \n        pred_num = extract_answer_num(completion)\n        score = 0.0\n        \n        # 1. CORRECTNESS (main signal)\n        if gold_num is not None and pred_num is not None:\n            if abs(pred_num - gold_num) < 1e-3:\n                score += 3.0  # Correct answer\n                if i == 0 and _ITER[0] % 50 == 0:\n                    print(f\"[Step {_ITER[0]}] CORRECT: pred={pred_num}, gold={gold_num}\")\n            else:\n                score -= 0.5  # Wrong answer penalty\n        else:\n            score -= 0.3  # No answer extracted\n        \n        # 2. FORMAT (strict)\n        has_reasoning = '<reasoning>' in completion and '</reasoning>' in completion\n        has_answer = '<answer>' in completion and '</answer>' in completion\n        if has_reasoning and has_answer:\n            score += 2.0  # Full format compliance\n        \n        # 3. FORMAT (soft - partial credit)\n        score += 0.1 if '<reasoning>' in completion else 0\n        score += 0.1 if '</reasoning>' in completion else 0\n        score += 0.1 if '<answer>' in completion else 0\n        score += 0.1 if '</answer>' in completion else 0\n        \n        # 4. REASONING QUALITY\n        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', completion, re.DOTALL)\n        if reasoning_match:\n            reasoning_text = reasoning_match.group(1)\n            if len(reasoning_text) > 50 and check_ascii_math(reasoning_text):\n                score += 0.5  # Good reasoning\n        \n        rewards.append(score)\n    \n    return rewards\n\nprint(\"Reward function ready\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GRPO setup\nGRPO_PROMPT = \"<start_of_turn>user\\nSolve step-by-step. Use <reasoning> for work, <answer> for final answer.\\n\\n{q}<end_of_turn>\\n<start_of_turn>model\"\n\ngrpo_formatted = [{\"prompts\": GRPO_PROMPT.format(q=x[\"q\"]), \"question\": x[\"q\"], \"answer\": x[\"gold\"]} for x in grpo_samples]\n\n# 2 epochs = repeat dataset\ngrpo_data_2epochs = grpo_formatted * GRPO_EPOCHS\ngrpo_ds = grain.MapDataset.source(grpo_data_2epochs).batch(GRPO_BATCH)\nGRPO_STEPS = len(grpo_ds)\n\nprint(f\"GRPO: {len(grpo_samples)} x {GRPO_EPOCHS} epochs = {len(grpo_data_2epochs)} samples, {GRPO_STEPS} steps\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GRPO training - With Reward Logging\ndel base_model\ngc.collect()\n\n# === Reward Logging Callback ===\nclass RewardLogger:\n    def __init__(self):\n        self.rewards = []\n        self.steps = []\n    \n    def log(self, step, reward):\n        self.steps.append(step)\n        self.rewards.append(reward)\n        print(f\"Step {step}: Reward = {reward:.4f}\")\n\nreward_logger = RewardLogger()\n\n# Wrap reward function to log values\ndef gsm8k_reward_with_logging(prompts, completions, **kwargs):\n    rewards = gsm8k_reward_final(prompts, completions, **kwargs)\n    avg_reward = float(jnp.mean(rewards)) if hasattr(rewards, 'mean') else sum(rewards)/len(rewards)\n    print(f\"  → Batch Reward: {avg_reward:.4f}\")\n    return rewards\n\nopt = optax.chain(\n    optax.clip_by_global_norm(0.1),\n    optax.adamw(optax.schedules.warmup_cosine_decay_schedule(0, GRPO_LR, int(0.1*GRPO_STEPS), GRPO_STEPS, 0), b1=0.9, b2=0.99, weight_decay=0.1)\n)\n\ncluster_cfg = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={r: mesh for r in [rl_cluster_lib.Role.ACTOR, rl_cluster_lib.Role.REFERENCE, rl_cluster_lib.Role.ROLLOUT]},\n    rollout_engine='vanilla',\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=opt,\n        max_steps=GRPO_STEPS,\n        mini_batch_size=GRPO_BATCH,\n        checkpoint_root_directory=str(GRPO_CKPT),\n        eval_every_n_steps=100  \n    ),\n    rollout_config=base_rollout.RolloutConfig(max_tokens_to_generate=512, max_prompt_length=256, kv_cache_size=1024, temperature=0.9, top_k=50)\n)\n\ncluster = rl_cluster_lib.RLCluster(\n    actor=model,\n    reference=model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_cfg\n)\n\n# ✅ Use wrapped reward function with logging\ntrainer = GRPOLearner(\n    rl_cluster=cluster,\n    reward_fns=[gsm8k_reward_with_logging],  # Changed!\n    grpo_config=GRPOConfig(num_generations=NUM_GENS, beta=KL_COEF)\n)\n\nprint(f\"--- GRPO: {GRPO_STEPS} steps, {GRPO_EPOCHS} epochs ---\")\nwith mesh:\n    trainer.train(grpo_ds)\nprint(\"GRPO complete\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nfinal = GRPO_CKPT / \"final\"\nckptr.save(str(final), nnx.state(model)); ckptr.wait_until_finished()\n\nzip_out = WORK_DIR / \"submission.zip\"\nwith zipfile.ZipFile(zip_out, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(final):\n        for f in files: zf.write(os.path.join(root, f), os.path.relpath(os.path.join(root, f), final))\nprint(f\"Saved: {zip_out}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}